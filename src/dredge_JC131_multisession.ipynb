{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot drift across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import labdatatools as ldt\n",
    "import spks\n",
    "from natsort import natsorted\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data from GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT = 'JC131'\n",
    "DATATYPE = 'kilosort2.5'\n",
    "DPATH = Path(ldt.labdata_preferences['paths'][0])\n",
    "SAVEPATH = Path.home() / 'chronic_manuscript_figures'\n",
    "\n",
    "files = ldt.rclone_list_files(SUBJECT)\n",
    "files.head()\n",
    "\n",
    "ori_sessions = files[files.datatype == 'orientation'].session\n",
    "ori_sessions = np.unique(ori_sessions).tolist()\n",
    "\n",
    "temp = []\n",
    "for sess in ori_sessions: # ensure that kilosort data exists\n",
    "    if DATATYPE in files[files.session == sess].datatype.values:\n",
    "        ses = files[files.session == sess].iloc[0].session\n",
    "        temp.append(ses)\n",
    "ori_sessions = np.array(temp)\n",
    "print(len(ori_sessions))\n",
    "\n",
    "ori_sessions = natsorted(ori_sessions)\n",
    "ori_sessions = ori_sessions[:-1] #discard session that was taken way later\n",
    "ori_sessions = np.array(ori_sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionfolder_to_datetime(folder,folderformat = '%Y%m%d_%H%M%S'):\n",
    "    dd = datetime.datetime.strptime(folder,folderformat)\n",
    "    return dd #dd.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "datetime_scale = [sessionfolder_to_datetime(f) for f in ori_sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ori_sessions = ori_sessions[:5] #truncate for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for date in ori_sessions:\n",
    "#    ldt.rclone_get_data(subject=SUBJECT, session=date, datatype=DATATYPE, excludes=['**.bin']) #spike sorting\n",
    "#    ldt.rclone_get_data(subject=SUBJECT, session=date, includes=['**imec*.ap.meta'], excludes=['**.bin']) #meta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop sessions with a different channelmap\n",
    "from spks import *\n",
    "\n",
    "coords = None\n",
    "keep_idx = []\n",
    "metapaths = []\n",
    "for i,date in enumerate(ori_sessions):\n",
    "    metapath = Path(ldt.get_filepath(subject=SUBJECT, session=date, subfolders=['ephys_*','*'], filename='*imec*.ap.meta'))\n",
    "    #print(metapath)\n",
    "    meta = read_spikeglx_meta(metapath)\n",
    "    if coords is None:\n",
    "        coords = meta['coords']\n",
    "    if np.array_equal(coords, meta['coords']):\n",
    "        keep_idx.append(i)\n",
    "        metapaths.append(metapath)\n",
    "    else:\n",
    "        print(f'Dropping {ori_sessions[i]} due to different channelmap')\n",
    "\n",
    "\n",
    "ori_sessions = ori_sessions[keep_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load spike positions, amplitudes, and depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clusters(kilosort_paths):\n",
    "    clus = []\n",
    "    for p in kilosort_paths:\n",
    "        print(p)\n",
    "        clus.append(Clusters(p, load_template_features=True))\n",
    "    return clus\n",
    "\n",
    "def compute_session_offsets_and_srate(metapaths):\n",
    "    fileoffset_seconds = 0\n",
    "    session_breaks = [0]\n",
    "    srates = []\n",
    "    for m in metapaths:\n",
    "        meta = read_spikeglx_meta(m)\n",
    "        srates.append(meta['sRateHz'])\n",
    "        fileoffset_seconds += meta['fileTimeSecs']\n",
    "        session_breaks.append(fileoffset_seconds)\n",
    "    return session_breaks, srates\n",
    "\n",
    "def get_subset_of_spikes(clus,\n",
    "                         srates,\n",
    "                         shank_num,\n",
    "                         mindepth=None,\n",
    "                         maxdepth=None,\n",
    "                         spike_fraction=None,\n",
    "                         t_start_sec=0,\n",
    "                         t_end_sec=None):\n",
    "    amps, depths, times = [],[],[]\n",
    "    session_breaks = [0]\n",
    "    fileoffset_seconds = 0\n",
    "    for i,c in enumerate(clus):\n",
    "        depth = c.spike_positions[:,1]\n",
    "        amp = np.abs(c.spike_amplitudes)\n",
    "        spike_times = c.spike_times / srates[i]\n",
    "        cluster_ids_on_desired_shank = c.cluster_info[c.cluster_info.shank == shank_num].cluster_id.values\n",
    "        spikes_on_desired_shank = np.isin(c.spike_clusters, cluster_ids_on_desired_shank)\n",
    "\n",
    "        valid_time_indices = spike_times >= t_start_sec\n",
    "        spike_times = spike_times[valid_time_indices]\n",
    "        amp = amp[valid_time_indices]\n",
    "        depth = depth[valid_time_indices]\n",
    "        spikes_on_desired_shank = spikes_on_desired_shank[valid_time_indices]\n",
    "\n",
    "        if t_end_sec is not None:\n",
    "            valid_time_indices = spike_times <= t_end_sec\n",
    "            spike_times = spike_times[valid_time_indices]\n",
    "            amp = amp[valid_time_indices]\n",
    "            depth = depth[valid_time_indices]\n",
    "            spikes_on_desired_shank = spikes_on_desired_shank[valid_time_indices]\n",
    "        \n",
    "        spike_times = spike_times - t_start_sec # make spike times start at zero\n",
    "        spike_times += fileoffset_seconds\n",
    "\n",
    "\n",
    "        amp = amp[spikes_on_desired_shank]\n",
    "        depth = depth[spikes_on_desired_shank]\n",
    "        spike_times = spike_times[spikes_on_desired_shank]\n",
    "\n",
    "        #not_noise  = (clu.cluster_info.active_channels)<40  #TODO: do something to throw out noise spikes?\n",
    "\n",
    "        #print(np.min(depth))\n",
    "        #print(np.max(depth))\n",
    "        if maxdepth is not None:\n",
    "            amp = amp[depth < maxdepth]\n",
    "            spike_times = spike_times[depth < maxdepth]\n",
    "            depth = depth[depth < maxdepth]\n",
    "\n",
    "\n",
    "        if mindepth is not None:\n",
    "            amp = amp[depth > mindepth]\n",
    "            spike_times = spike_times[depth > mindepth]\n",
    "            depth = depth[depth > mindepth]\n",
    "\n",
    "        if spike_fraction is not None:\n",
    "            subselect_inds = np.random.choice(np.arange(len(amps)), len(amps) // spike_fraction, replace=False)\n",
    "            amp = amp[subselect_inds]\n",
    "            spike_times = spike_times[subselect_inds]\n",
    "            depth = depth[subselect_inds]\n",
    "\n",
    "        amps.extend(amp)\n",
    "        depths.extend(depth)\n",
    "        times.extend(spike_times)\n",
    "\n",
    "        if not len(spike_times):\n",
    "            spike_times = [t_end_sec-t_start_sec]\n",
    "        fileoffset_seconds = spike_times[-1] + 1e-6\n",
    "        session_breaks.append(fileoffset_seconds)\n",
    "\n",
    "\n",
    "    amps = np.stack(amps)\n",
    "    times = np.stack(times)\n",
    "    depths = np.stack(depths)\n",
    "\n",
    "    assert amps.shape == depths.shape == times.shape\n",
    "    return amps, times, depths, session_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: more memory intense\n",
    "kilosort_paths = [DPATH / SUBJECT / date / DATATYPE / 'imec0' for date in ori_sessions]\n",
    "\n",
    "clus = get_all_clusters(kilosort_paths)\n",
    "session_breaks, srates = compute_session_offsets_and_srate(metapaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dredge.dredge_ap import register\n",
    "from spks.viz import plot_drift_raster\n",
    "\n",
    "for shank in [0,1,2,3]:\n",
    "    amps, times, depths, session_breaks = get_subset_of_spikes(clus, srates, shank, mindepth=4300, maxdepth=4900, t_start_sec=60, t_end_sec=300)\n",
    "    motion_est, _ = register(amps, depths, times)\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plot_drift_raster(times, depths, amps, rasterized=True)\n",
    "    plt.vlines(session_breaks, *plt.gca().get_ylim(), linestyles='--', colors='black')\n",
    "    #plt.plot(motion_est.spatial_bin_centers_um + motion_est.displacement.T)\n",
    "    plt.plot(4650 + motion_est.displacement.T)\n",
    "\n",
    "    filename = SAVEPATH / f'motion_over_days_shank_{shank}_V1.pdf'\n",
    "    plt.savefig(filename, format='pdf',dpi=500, bbox_inches='tight')\n",
    "    \n",
    "    amps, times, depths, session_breaks = get_subset_of_spikes(clus, srates, shank, mindepth=1500, maxdepth=2100, t_start_sec=60, t_end_sec=300)\n",
    "    motion_est, _ = register(amps, depths, times)\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plot_drift_raster(times, depths, amps, rasterized=True)\n",
    "    plt.vlines(session_breaks, *plt.gca().get_ylim(), linestyles='--', colors='black')\n",
    "    #plt.plot(motion_est.spatial_bin_centers_um + motion_est.displacement.T)\n",
    "    plt.plot(1800 + motion_est.displacement.T)\n",
    "\n",
    "    filename = SAVEPATH / f'motion_over_days_shank_{shank}_thalamus.pdf'\n",
    "    plt.savefig(filename, format='pdf',dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## method 2: slower\n",
    "#kilosort_paths = [DPATH / SUBJECT / date / DATATYPE / 'imec0' for date in ori_sessions]\n",
    "#SHANK = 0\n",
    "#amps, times, depths, session_breaks = get_all_spikes(kilosort_paths, metapaths, SHANK, mindepth=3000, t_start_sec=60, t_end_sec=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dredge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
